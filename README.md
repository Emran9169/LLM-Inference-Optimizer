# ğŸš€ LLM Inference Optimizer  
**Optimizing GPT-2 Inference with PyTorch, ONNX, and INT8 Quantization**  

---

## ğŸ“– Project Overview  
This project explores how to **optimize Large Language Model (LLM) inference** by:  
1. Running a **baseline GPT-2 model in PyTorch**.  
2. Exporting GPT-2 to the **ONNX format** for accelerated inference.  
3. Applying **INT8 quantization** to improve performance and reduce memory usage.  
4. Benchmarking all approaches and comparing results.  

The goal is to demonstrate practical **model optimization and deployment techniques** that improve inference speed without sacrificing too much accuracy.  

---

## âš™ï¸ Tech Stack  
- **Python 3.10**  
- **PyTorch** â€“ Baseline model inference  
- **ONNX Runtime** â€“ Optimized execution engine  
- **ONNX Quantization Toolkit** â€“ INT8 dynamic quantization  
- **Transformers (Hugging Face)** â€“ Tokenizer & GPT-2 model  
- **Pandas / CSV** â€“ Benchmark result storage  

---

## ğŸ“Š Benchmark Results  

| Engine       | Mean Latency (ms) | P95 Latency (ms) | Tokens/sec |
|--------------|------------------|------------------|------------|
| **PyTorch**  | ~2000+           | ~2100+           | ~30â€“40     |
| **ONNX**     | 1112.89          | 1172.04          | 57.51      |
| **ONNX INT8**| 484.36           | 500.73           | 132.13     |

âœ… **2.3Ã— faster** ONNX over PyTorch  
âœ… **~4.5Ã— faster** ONNX INT8 over PyTorch  

---

## ğŸ“‚ Project Structure

'''
llm-inference-optimizer/

â”‚â”€â”€ src/
â”‚ â”œâ”€â”€ baseline_pytorch.py # Run baseline GPT-2 with PyTorch
â”‚ â”œâ”€â”€ export_onnx.py # Export model to ONNX
â”‚ â”œâ”€â”€ bench_onnx.py # Run ONNX inference benchmark
â”‚ â”œâ”€â”€ bench_onnx_int8.py # Quantize & benchmark INT8 model
â”‚
â”‚â”€â”€ results/
â”‚ â”œâ”€â”€ baseline_pytorch.csv # PyTorch results
â”‚ â”œâ”€â”€ onnx_baseline.csv # ONNX results
â”‚ â”œâ”€â”€ onnx_int8.csv # ONNX INT8 results
â”‚
â”‚â”€â”€ models/ # (ignored) contains exported ONNX models
â”‚â”€â”€ requirements.txt
â”‚â”€â”€ README.md
'''
