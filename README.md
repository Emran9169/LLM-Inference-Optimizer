# üöÄ LLM Inference Optimizer  
**Optimizing GPT-2 Inference with PyTorch, ONNX, and INT8 Quantization**  

---

## üìñ Project Overview  
This project explores how to **optimize Large Language Model (LLM) inference** by:  
1. Running a **baseline GPT-2 model in PyTorch**.  
2. Exporting GPT-2 to the **ONNX format** for accelerated inference.  
3. Applying **INT8 quantization** to improve performance and reduce memory usage.  
4. Benchmarking all approaches and comparing results.  

The goal is to demonstrate practical **model optimization and deployment techniques** that improve inference speed without sacrificing too much accuracy.  

---

## ‚öôÔ∏è Tech Stack  
- **Python 3.10**  
- **PyTorch** ‚Äì Baseline model inference  
- **ONNX Runtime** ‚Äì Optimized execution engine  
- **ONNX Quantization Toolkit** ‚Äì INT8 dynamic quantization  
- **Transformers (Hugging Face)** ‚Äì Tokenizer & GPT-2 model  
- **Pandas / CSV** ‚Äì Benchmark result storage  

---

## üìä Benchmark Results  

| Engine       | Mean Latency (ms) | P95 Latency (ms) | Tokens/sec |
|--------------|------------------|------------------|------------|
| **PyTorch**  | ~2000+           | ~2100+           | ~30‚Äì40     |
| **ONNX**     | 1112.89          | 1172.04          | 57.51      |
| **ONNX INT8**| 484.36           | 500.73           | 132.13     |

‚úÖ **2.3√ó faster** ONNX over PyTorch  
‚úÖ **~4.5√ó faster** ONNX INT8 over PyTorch  

---

## üìÇ Project Structure  
